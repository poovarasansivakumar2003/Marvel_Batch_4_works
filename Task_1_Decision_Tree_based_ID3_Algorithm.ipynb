{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOS7qYCHckgnKXjVRfxFl0R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poovarasansivakumar2003/Marvel_works/blob/main/Task_1_Decision_Tree_based_ID3_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Trees: An Overview\n",
        "\n",
        "#### Decision Trees and Terminology\n",
        "\n",
        "A decision tree is a powerful tool used in machine learning for classification and regression tasks. It mimics the human decision-making process by breaking down data into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.\n",
        "\n",
        "- **Root Node**: The top node of a decision tree that represents the entire dataset, which is then split into subsets.\n",
        "- **Decision Node**: A node that splits into further sub-nodes based on certain conditions.\n",
        "- **Leaf Node (Terminal Node)**: Nodes that do not split further, representing the final decision or classification.\n",
        "- **Branches**: Links between nodes that represent the decision paths taken.\n",
        "\n",
        "#### Entropy and Information Gain\n",
        "\n",
        "**Entropy** measures the amount of uncertainty or impurity in a dataset. It is given by the formula:\n",
        "\n",
        "![Entropy](https://miro.medium.com/v2/resize:fit:640/format:webp/1*voRgqFZhfko4ZfG3odSK_Q.png)\n",
        "\n",
        "where pi is the proportion of each category i=1,â€¦,N.\n",
        "\n",
        "**Information Gain** quantifies the reduction in entropy achieved by partitioning the dataset according to an attribute. It is the difference between the entropy of the dataset before and after the split:\n",
        "\n",
        "![IG](https://miro.medium.com/v2/resize:fit:640/format:webp/1*_uzMwgApeQPXEylcwOc3WQ.png)\n",
        "\n",
        "where j is particular feature.\n",
        "\n",
        "#### ID3 Algorithm\n",
        "\n",
        "ID3 (Iterative Dichotomiser 3) is a simple decision tree algorithm introduced by Ross Quinlan in 1986. It uses entropy and information gain to construct a decision tree by recursively partitioning the dataset.\n",
        "\n",
        "##### Procedure for ID3\n",
        "\n",
        "1. **Calculate Entropy** of the entire dataset.\n",
        "2. **For each attribute**, calculate the information gain when the dataset is split by that attribute.\n",
        "3. **Select the attribute** with the highest information gain as the decision node.\n",
        "4. **Split the dataset** into subsets based on the chosen attribute.\n",
        "5. **Repeat recursively** for each subset, using only the remaining attributes.\n",
        "6. **Stop** when all instances in a subset belong to the same class or no more attributes are available.\n"
      ],
      "metadata": {
        "id": "A6GNOjLU2JsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from collections import Counter\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, attribute=None, threshold=None, left=None, right=None, *, value=None):\n",
        "        self.attribute = attribute\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "def entropy(y):\n",
        "    proportions = np.bincount(y) / len(y)\n",
        "    return -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
        "\n",
        "def information_gain(X, y, attribute, threshold):\n",
        "    parent_entropy = entropy(y)\n",
        "    left_idx = X[:, attribute] <= threshold\n",
        "    right_idx = X[:, attribute] > threshold\n",
        "    if np.sum(left_idx) == 0 or np.sum(right_idx) == 0:\n",
        "        return 0\n",
        "    n, n_left, n_right = len(y), np.sum(left_idx), np.sum(right_idx)\n",
        "    left_entropy, right_entropy = entropy(y[left_idx]), entropy(y[right_idx])\n",
        "    child_entropy = (n_left / n) * left_entropy + (n_right / n) * right_entropy\n",
        "    return parent_entropy - child_entropy\n",
        "\n",
        "def best_split(X, y):\n",
        "    best_gain = 0\n",
        "    best_attribute, best_threshold = None, None\n",
        "    for attribute in range(X.shape[1]):\n",
        "        thresholds = np.unique(X[:, attribute])\n",
        "        for threshold in thresholds:\n",
        "            gain = information_gain(X, y, attribute, threshold)\n",
        "            if gain > best_gain:\n",
        "                best_gain, best_attribute, best_threshold = gain, attribute, threshold\n",
        "    return best_attribute, best_threshold\n",
        "\n",
        "def build_tree(X, y, depth=0, max_depth=None):\n",
        "    if len(set(y)) == 1:\n",
        "        return Node(value=y[0])\n",
        "    if max_depth is not None and depth >= max_depth:\n",
        "        return Node(value=Counter(y).most_common(1)[0][0])\n",
        "    attribute, threshold = best_split(X, y)\n",
        "    if attribute is None:\n",
        "        return Node(value=Counter(y).most_common(1)[0][0])\n",
        "    left_idx = X[:, attribute] <= threshold\n",
        "    right_idx = X[:, attribute] > threshold\n",
        "    left = build_tree(X[left_idx], y[left_idx], depth + 1, max_depth)\n",
        "    right = build_tree(X[right_idx], y[right_idx], depth + 1, max_depth)\n",
        "    return Node(attribute=attribute, threshold=threshold, left=left, right=right)\n",
        "\n",
        "def predict_tree(tree, X):\n",
        "    if tree.value is not None:\n",
        "        return tree.value\n",
        "    if X[tree.attribute] <= tree.threshold:\n",
        "        return predict_tree(tree.left, X)\n",
        "    else:\n",
        "        return predict_tree(tree.right, X)\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Build the decision tree\n",
        "tree = build_tree(X, y, max_depth=3)\n",
        "\n",
        "# Predict on the dataset\n",
        "predictions = [predict_tree(tree, x) for x in X]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.sum(predictions == y) / len(y)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix0m7XNi6T2s",
        "outputId": "37c50c5a-f854-4d74-c03d-01f7ca7b2fbd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 97.33%\n"
          ]
        }
      ]
    }
  ]
}