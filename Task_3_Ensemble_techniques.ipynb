{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdNeJ3S+wJxI4q8XOJV7aH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poovarasansivakumar2003/Marvel_Batch_4_works/blob/main/Task_3_Ensemble_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Techniques:\n",
        "\n",
        "##What are Ensemble Techniques?\n",
        "\n",
        "Ensemble techniques in machine learning involve combining multiple base models to produce a single, powerful model. The idea is to leverage the strengths of various models to achieve better performance than any single model alone. Ensemble methods can reduce overfitting, improve accuracy, and provide more robust predictions.\n",
        "\n",
        "##Types of Ensemble Methods\n",
        "\n",
        "**Bagging (Bootstrap Aggregating)**: Involves training multiple instances of the same model on different subsets of the training data (with replacement).\n",
        "Each model votes, and the final prediction is based on the majority vote or average.\n",
        "Example: Random Forest.\n",
        "\n",
        "**Boosting**: Involves training multiple models sequentially, each trying to correct the errors of the previous one.\n",
        "Models are trained on the weighted data, giving more focus to incorrectly predicted instances.\n",
        "Example: AdaBoost, Gradient Boosting.\n",
        "\n",
        "**Stacking**: Involves training multiple models (base learners) and then using another model (meta-learner) to combine their predictions.\n",
        "The base models are trained on the initial dataset, and the meta-learner is trained on their predictions.\n",
        "\n",
        "**Voting**: Combines the predictions of multiple models by averaging (for regression) or taking a majority vote (for classification).\n",
        "Models can be assigned different weights based on their performance.\n",
        "\n",
        "**Blending**: Similar to stacking but involves a holdout validation set instead of cross-validation for training the meta-learner.\n",
        "\n",
        "##Advantages of Ensemble Methods\n",
        "\n",
        "**Improved Accuracy**: By combining multiple models, ensemble methods can achieve higher accuracy than individual models.\n",
        "\n",
        "**Robustness**: They are less likely to overfit the training data and generalize better to unseen data.\n",
        "\n",
        "**Versatility**: They can be applied to both classification and regression tasks.\n",
        "\n",
        "##Implementation\n",
        "\n",
        "Applying Ensemble Techniques on the Titanic Dataset\n",
        "\n",
        "The Titanic dataset is a well-known dataset for binary classification tasks (survival prediction). We will demonstrate the use of various ensemble techniques on this dataset.\n",
        "\n",
        "Let's start with importing necessary libraries and loading the dataset."
      ],
      "metadata": {
        "id": "CapJ8HtShpgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Data preprocessing\n",
        "data = data.drop(['Cabin', 'Ticket', 'Name'], axis=1)\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "data['Fare'].fillna(data['Fare'].median(), inplace=True)\n",
        "\n",
        "# Encode categorical features\n",
        "label_encoder = LabelEncoder()\n",
        "data['Sex'] = label_encoder.fit_transform(data['Sex'])\n",
        "data['Embarked'] = label_encoder.fit_transform(data['Embarked'])\n",
        "\n",
        "# Define features and target\n",
        "X = data.drop(['Survived'], axis=1)\n",
        "y = data['Survived']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base learners\n",
        "clf1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf2 = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "clf3 = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Voting classifier (hard voting)\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('rf', clf1),\n",
        "    ('ada', clf2),\n",
        "    ('gb', clf3)], voting='hard')\n",
        "\n",
        "# Train the voting classifier\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "print(f'Voting Classifier Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXmJDSrKizM3",
        "outputId": "38cf7eba-eb5f-4a4b-fc54-fab634050ace"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Classifier Accuracy: 0.8101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Explanation\n",
        "\n",
        "**Data Preprocessing**:\n",
        "<ul>\n",
        "<li>Dropped unnecessary columns.</li>\n",
        "<li>Filled missing values.</li>\n",
        "<li>Encoded categorical variables.</li>\n",
        "</ul>\n",
        "\n",
        "**Base Learners**:\n",
        "<ul>\n",
        "<li>Random Forest, AdaBoost, and Gradient Boosting classifiers were chosen as base learners.</li>\n",
        "</ul>\n",
        "\n",
        "**Voting Classifier**:\n",
        "<ul>\n",
        "<li>Combined the predictions of the base learners using hard voting.</li>\n",
        "<li>Trained the voting classifier on the training data.</li>\n",
        "<li>Evaluated its performance on the test data.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "f_euNQKejMkL"
      }
    }
  ]
}